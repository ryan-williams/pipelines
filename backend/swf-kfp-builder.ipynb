{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Kubeflow Pipelines as `ScheduledWorkflow`s\n",
    "This notebook shows examples of submitting Kubeflow [`ScheduledWorkflow`s](https://github.com/kubeflow/pipelines/blob/0.1.31/backend/src/crd/pkg/apis/scheduledworkflow/register.go) (SWFs) that run [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/) (KFPs).\n",
    "\n",
    "It contains helper-functions for easy fetching and wrapping of KFPs in SWFs, as well as [examples demonstrating these capabilities](#examples).\n",
    "\n",
    "## Wrapping a Kubeflow Pipeline in a `ScheduledWorkflow`\n",
    "Conceptually, a \"scheduled\" Kubeflow Pipeline must take exactly one argument: the timestamp it was scheduled to run at. Any other parameters it normally accepts must be [partially-applied](https://en.wikipedia.org/wiki/Partial_application) away (i.e. assigned specific, concrete values), leaving just the timestamp parameter to be filled by the SWF machinery on each pipeline instantiation. \n",
    "\n",
    "## \n",
    "\n",
    "The timestamp passed by the SWF controller to each KFP instance it spawns will generally be one of two types:\n",
    "- a timestamp close to the current time (when the SWF is caught up to the present, and spawning KFPs according to a `crontab`-style schedule or fixed interval)\n",
    "- a timestamp in the past (when the SWF is created with a \"start\" time in the past, and is back-filling KFP runs to catch up to the present)\n",
    "\n",
    "\n",
    "## TODOs\n",
    "- [ ] factor out helpers\n",
    "- tests:\n",
    "  - example pipelines:\n",
    "      - [ ] `gs://ml-pipeline-playground/coin.tar.gz`\n",
    "      - [ ] [`https://storage.googleapis.com/ml-pipeline-playground/coin.tar.gz`](https://storage.googleapis.com/ml-pipeline-playground/coin.tar.gz)\n",
    "      - [x] [`https://raw.githubusercontent.com/kubeflow/pipelines/0.1.31/samples/core/condition/condition.py`](https://raw.githubusercontent.com/kubeflow/pipelines/0.1.31/samples/core/condition/condition.py)\n",
    "      - [ ] `component.yaml` example(s) (TODO: which one(s)?)\n",
    "      - [ ] Bitcoin BigQuery monthly rollups\n",
    "      - [ ] Current Weather (OpenWeather)\n",
    "      - [ ] Stock movements (IEX API)\n",
    "  - scheduling:\n",
    "      - [ ] intervals\n",
    "      - [x] start/end times\n",
    "      - [ ] updating/clearing time-bounds\n",
    "- [x] `!pip install` requirements\n",
    "- [ ] (optionally) pass scheduled time to pipeline\n",
    "- [ ] verify provided pipelines take no arguments (other than e.g. scheduled datetime)\n",
    "- [ ] parameterize notebook w/ `papermill`\n",
    "- [ ] support wrapping `component.yaml`s into Pipelines and running\n",
    "  - [ ] verify no extra arguments to provided components\n",
    "- [ ] publish runner container publicly\n",
    "- [ ] publish SWF+KFP YAML template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install KFP SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install kfp==0.1.31 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg_resources import get_distribution\n",
    "kfp_version = get_distribution(\"kfp\").version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "Below are utilities for:\n",
    "- [fetching/loading pipelines](#Pipeline-fetching/loading)\n",
    "- [parsing their YAML definitions](#Pipeline-spec-accessors)\n",
    "- [building `ScheduledWorkflow` resources](#ScheduledWorkflow-builders)\n",
    "- [`import`ing remote `.py` files](#importing-remote-modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline fetching/loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_stream(url):\n",
    "    \"\"\"Return a file-like-object from a local or remote (\"gs\" or \"https?\") URL\"\"\"\n",
    "    from urllib.parse import urlparse\n",
    "    parsed = urlparse(url)\n",
    "    scheme = parsed.scheme\n",
    "    if not scheme:\n",
    "        return open(url, 'rb')\n",
    "    elif scheme == 'http' or scheme == 'https':\n",
    "        from urllib.request import urlopen\n",
    "        return urlopen(url)\n",
    "    elif scheme == 'gs':\n",
    "        from google.cloud import storage\n",
    "        gcs = storage.Client()\n",
    "        bucket_name = parsed.hostname\n",
    "        bucket = gcs.get_bucket(bucket_name)\n",
    "        path = parsed.path\n",
    "        blob = bucket.blob(path)\n",
    "        from tempfile import NamedTemporaryFile\n",
    "        with NamedTemporaryFile() as f:\n",
    "            blob.download_to_file(f.name)\n",
    "            return open(f, 'rb')\n",
    "    else:\n",
    "        raise Exception(\"Unsure how to handle URL scheme '%s' (%s)\" % (scheme, url))\n",
    "\n",
    "def url_to_bytes(url):\n",
    "    \"\"\"Return the contents of a local or remote (\"gs\" or \"https?\") URL\"\"\"\n",
    "    with url_to_stream(url) as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_extract_pipeline_tar(bytes):\n",
    "    \"\"\"Attempt to parse `bytes` as a TAR archive and extract a `pipeline.yaml`\"\"\"\n",
    "    import tarfile\n",
    "    from tarfile import TarError\n",
    "    try:\n",
    "        from io import BytesIO\n",
    "        with tarfile.open(fileobj=BytesIO(bytes), mode='r') as f:\n",
    "            names = f.getnames()\n",
    "            if names == ['pipeline.yaml']:\n",
    "                tar_info = f.getmember('pipeline.yaml')\n",
    "                if tar_info.isfile():\n",
    "                    return f.extractfile(tar_info).read()\n",
    "                raise Exception('\"pipeline.yaml\" in TAR archive is not a regular file')\n",
    "            raise Exception('Expected TAR archive to contain only a \"pipeline.yaml\"; found %s' % names)\n",
    "    except TarError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_extract_pipeline_zip(bytes):\n",
    "    \"\"\"Attempt to parse `bytes` as a ZIP archive and extract a `pipeline.yaml`\"\"\"\n",
    "    from zipfile import BadZipfile, ZipFile\n",
    "    try:\n",
    "        from io import BytesIO\n",
    "        with ZipFile(BytesIO(bytes), mode='r') as f:\n",
    "            names = f.namelist()\n",
    "            if names == ['pipeline.yaml']:\n",
    "                return f.read('pipeline.yaml')\n",
    "            raise Exception('Expected ZIP archive to contain only a \"pipeline.yaml\"; found %s' % names)\n",
    "    except BadZipFile:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pipeline_func(pipeline):\n",
    "    return \\\n",
    "        hasattr(pipeline, '__call__') and \\\n",
    "        hasattr(pipeline, '_pipeline_name') and \\\n",
    "        hasattr(pipeline, '_pipeline_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline(pipeline):\n",
    "    \"\"\"Load a pipeline's spec as a dict\n",
    "    \n",
    "    Input pipeline can be:\n",
    "    - a @dsl.pipeline function (in which case it is compiled to YAML which is then parsed)\n",
    "    - a local or remote (\"gs\" and \"https?\" schemes supported) YAML file (or ZIP or TAR archive containing a pipeline.yaml)\n",
    "    \"\"\"\n",
    "    import yaml\n",
    "    if is_pipeline_func(pipeline):\n",
    "        from kfp.compiler import Compiler\n",
    "        compiler = Compiler()\n",
    "        pipeline_yaml = compiler.compile(pipeline, package_path=None)\n",
    "        return yaml.safe_load(pipeline_yaml)\n",
    "    bytes = url_to_bytes(pipeline)\n",
    "    pipeline_yaml = try_extract_pipeline_tar(bytes)\n",
    "    if pipeline_yaml is None:\n",
    "        pipeline_yaml = try_extract_pipeline_zip(bytes)\n",
    "        if pipeline_yaml is None:\n",
    "            pipeline_yaml = bytes\n",
    "    return yaml.safe_load(pipeline_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(path):\n",
    "    \"\"\"Load YAML from a local or remote URL\"\"\"\n",
    "    import yaml\n",
    "    return yaml.safe_load(url_to_bytes(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline-spec accessors\n",
    "Utilities for pulling various fields out of a Kubeflow Pipeline's spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(pipeline):\n",
    "    if 'metadata' not in pipeline:\n",
    "        raise Exception('No \"metadata\" found in pipeline')\n",
    "    return pipeline['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations(pipeline):\n",
    "    metadata = get_metadata(pipeline)\n",
    "    if 'annotations' not in metadata:\n",
    "        raise Exception('No \"annotations\" found in pipeline \"metadata\"')\n",
    "    annotations = metadata['annotations']\n",
    "    if 'pipelines.kubeflow.org/pipeline_spec' not in annotations:\n",
    "        raise Exception('\"pipelines.kubeflow.org/pipeline_spec\" not found in pipeline metadata.annotations')\n",
    "    annotations = json.loads(annotations['pipelines.kubeflow.org/pipeline_spec'])\n",
    "    return annotations['name'], annotations['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(pipeline):\n",
    "    metadata = get_metadata(pipeline)\n",
    "\n",
    "    if 'generateName' not in metadata:\n",
    "        raise Exception('No \"generateName\" found in pipeline metadata')\n",
    "\n",
    "    name = metadata['generateName']\n",
    "    if name[-1] == '-':\n",
    "        name = name[:-1]\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(pipeline):\n",
    "    (_, description) = get_annotations(pipeline)\n",
    "    return description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ScheduledWorkflow` builders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load template `ScheduledWorkflow`+KFP YAML\n",
    "To construct `ScheduledWorkflow`s below, we start with this template and fill in a few missing fields (pipeline spec, name, and description, as well as any other parameter overrides the user provides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWF_TEMPLATE_PATH = '/Users/ryan/c/pipelines/backend/src/crd/samples/scheduledworkflow/kfp.yaml'\n",
    "SWF_TEMPLATE = load_yaml(SWF_TEMPLATE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate `ScheduledWorkflow`s for running Kubeflow Pipelines\n",
    "`make_swf_kfp` generates the spec for a `ScheduledWorkflow` resource that will run a given KFP on a desired schedule.\n",
    "\n",
    "#### Parameters\n",
    "| Type | Name | Description |\n",
    "| :-:| :-:| :---|\n",
    "| `pipeline` | `str` | `dsl.pipeline` function or path to a file (or ZIP or TAR archive) containing the pipeline's YAML specification (paths can be to local files or `gs`- or `http`-schemed URLs). |\n",
    "| `name` | `str` | Name of the ScheduledWorkflow resource to create; constructed from the underlying pipeline's name by default. |\n",
    "| `description` | `str` | Description of the ScheduledWorkflow resource to create; constructed from the underlying pipeline's description by default. |\n",
    "| `cron` | `str` | Crontab-formatted string specifying the schedule the pipeline should be run on; if neither `cron` nor `intervalSecond` is provided, the `DEFAULT_CRON_SCHEDULE` above is used. At most one of `cron` and `intervalSecond` should be provided. |\n",
    "| `intervalSecond` | `int` | Interval at which to trigger runs of the provided pipeline. At most one of `cron` and `intervalSecond` should be provided. |\n",
    "| `start` | `datetime` \\| `str` | If provided, begin scheduling pipelines at this date+time (UTC is assumed if timezone is not made explicit) |\n",
    "| `end` | `datetime` \\| `str` | If provided, stop scheduling pipelines at this date+time (UTC is assumed if timezone is not made explicit) |\n",
    "| `maxHistory` | `int` | Maximum number of pipeline runs' histories to store (default: 10) |\n",
    "| `maxConcurrency` | `int` | Maximum number of concurrent runs of the pipeline (default: 10) |\n",
    "| `enabled` | `bool` | Whether the generated ScheduledWorkflow should be enabled when it is created (default: True) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CRON_SCHEDULE = \"1 * * * * *\"\n",
    "def make_swf_kfp(\n",
    "    pipeline, \n",
    "    name=None, description=None, \n",
    "    cron=None, intervalSecond=None, \n",
    "    start=None, end=None,\n",
    "    maxHistory=10, maxConcurrency=10, enabled=True\n",
    "):\n",
    "    \"\"\"Create a ScheduledWorkflow resource that will run a given pipeline on a desired schedule.\n",
    "    \n",
    "    :param str pipeline @dsl.pipeline function or path to a file (or ZIP or TAR archive) containing the pipeline's YAML specification (paths can be to local files or \"gs\"- or \"http\"-schemed URLs).\n",
    "    :param str name Name of the ScheduledWorkflow resource to create; constructed from the underlying pipeline's name by default.\n",
    "    :param str description Description of the ScheduledWorkflow resource to create; constructed from the underlying pipeline's description by default.\n",
    "    :param str cron Crontab-formatted string specifying the schedule the pipeline should be run on; if neither `cron` nor `intervalSecond` is provided, the `DEFAULT_CRON_SCHEDULE` above is used. At most one of `cron` and `intervalSecond` should be provided.\n",
    "    :param int intervalSecond Interval at which to trigger runs of the provided pipeline. At most one of `cron` and `intervalSecond` should be provided.\n",
    "    :param datetime|str start If provided, begin scheduling pipelines at this date+time (UTC is assumed if timezone is not made explicit)\n",
    "    :param datetime|str end If provided, stop scheduling pipelines at this date+time (UTC is assumed if timezone is not made explicit)\n",
    "    :param int maxHistory Maximum number of pipeline runs' histories to store (default: 10)\n",
    "    :param int maxConcurrency Maximum number of concurrent runs of the pipeline (default: 10)\n",
    "    :param bool enabled Whether the generated ScheduledWorkflow should be enabled when it is created (default: True)    \n",
    "    \"\"\"\n",
    "    pipeline = load_pipeline(pipeline)\n",
    "    \n",
    "    from copy import deepcopy\n",
    "    swf = deepcopy(SWF_TEMPLATE)\n",
    "    \n",
    "    spec = swf['spec']\n",
    "    \n",
    "    if name is None:\n",
    "        name = get_name(pipeline)\n",
    "    \n",
    "    if description is None:\n",
    "        description = get_description(pipeline)\n",
    "    \n",
    "    if (cron is not None) and (intervalSecond is not None):\n",
    "        raise Exception('At most one of {\"cron\",\"interval\"} should be provided; received cron %s, interval %s' % (cron, intervalSecond))\n",
    "    \n",
    "    if (cron is None) and (intervalSecond is None):\n",
    "        cron = DEFAULT_CRON_SCHEDULE\n",
    "        \n",
    "    def set_date(key, dt):\n",
    "        from dateutil.parser import parse\n",
    "        if dt is not None:\n",
    "            if isinstance(dt, str):\n",
    "                dt = parse(dt)\n",
    "            if dt.tzinfo is None:\n",
    "                from pytz import utc\n",
    "                dt = utc.localize(dt)\n",
    "            schedule[key] = dt\n",
    "\n",
    "    schedule = {}\n",
    "    set_date('startTime', start)\n",
    "    set_date('endTime', end)\n",
    "    \n",
    "    msg_parts = []  # accumulate pieces of scheduling metadata here (for inclusion in the ScheduledWorkflow's description)\n",
    "    trigger = {}\n",
    "    if cron is not None:\n",
    "        schedule['cron'] = cron\n",
    "        msg_parts.append('cron: %s' % cron)\n",
    "        trigger['cronSchedule'] = schedule\n",
    "    else:\n",
    "        schedule['intervalSecond'] = intervalSecond\n",
    "        msg_parts.append('interval: %ds' % intervalSecond)\n",
    "        trigger['periodicSchedule'] = schedule\n",
    "        \n",
    "    spec['enabled'] = enabled\n",
    "    spec['maxHistory'] = maxHistory\n",
    "    spec['maxConcurrency'] = maxConcurrency\n",
    "    spec['trigger'] = trigger\n",
    "\n",
    "    if start is not None:\n",
    "        msg_parts.append('start: %s' % str(start))\n",
    "    if end is not None:\n",
    "        msg_parts.append('end: %s' % str(end))\n",
    "\n",
    "    spec['description'] = 'ScheduledWorkflow (%s): %s' % (', '.join(msg_parts), description)\n",
    "\n",
    "    # Name for the SWF resource\n",
    "    swf_name = 'swf-%s' % name\n",
    "    spec['name'] = swf_name\n",
    "    metadata = swf['metadata']\n",
    "    metadata['name'] = swf_name\n",
    "\n",
    "    workflow = spec['workflow']\n",
    "\n",
    "    # Inline pipeline YAML into SWF YAML as a parameter to each run of the SWF, \n",
    "    # which will parse the pipeline and submit it\n",
    "    parameters = workflow['parameters']\n",
    "    import yaml\n",
    "    parameters[1]['value'] = yaml.dump(pipeline)\n",
    "    \n",
    "    workflow_spec = workflow['spec']\n",
    "    templates = workflow_spec['templates']\n",
    "    template = templates[0]\n",
    "    container = template['container']\n",
    "    args = container['args']\n",
    "\n",
    "    # populate \"name\" argument in template\n",
    "    assert args[-2:] == [ '--name', '' ], \"Unexpected final SWF args: %s\" % args[-2:]\n",
    "    args[-1] = name\n",
    "\n",
    "    return swf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `import`ing remote modules\n",
    "ContextManager used for downloading (or moving) a file to a temporary location and importing from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Import(object):\n",
    "    \"\"\"ContextManager used for downloading (or moving) a file to a temporary location and importing from it\"\"\"\n",
    "    import sys\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.tmpdir = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        from urllib.parse import urlparse\n",
    "        url = self.url\n",
    "        parsed = urlparse(url)\n",
    "        scheme = parsed.scheme\n",
    "\n",
    "        from tempfile import TemporaryDirectory\n",
    "        self.tmpdir = TemporaryDirectory()\n",
    "\n",
    "        from os.path import basename, join\n",
    "        path = parsed.path\n",
    "        name = basename(path)  # Preserve file's basename (for ease of importing from it)\n",
    "        tmpdir = self.tmpdir.name\n",
    "        dest = join(tmpdir, name)  # File to import from will be downloaded here\n",
    "        \n",
    "        if not scheme:\n",
    "            # Local file: copy it to temporary directory (for consistent import-isolation semantics with remote \"import\"s)\n",
    "            with open(url, 'rb') as src, open(dest, 'wb') as dst:\n",
    "                from shutil import copyfileobj\n",
    "                copyfileobj(src, dst)\n",
    "        elif scheme == 'http' or scheme == 'https':\n",
    "            from urllib.request import urlretrieve\n",
    "            urlretrieve(url, dest)\n",
    "        elif scheme == 'gs':\n",
    "            from google.cloud import storage\n",
    "            gcs = storage.Client()\n",
    "            bucket_name = parsed.hostname\n",
    "            bucket = gcs.get_bucket(bucket_name)\n",
    "            key = parsed.path\n",
    "            blob = bucket.blob(key)\n",
    "            blob.download_to_file(dest)\n",
    "        else:\n",
    "            raise Exception(\"Unsure how to handle URL scheme '%s' (%s)\" % (scheme, url))    \n",
    "\n",
    "        # Add temporary directory to \"import\" path\n",
    "        sys.path.append(tmpdir)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # Close+Delete the temporary directory\n",
    "        self.tmpdir.__exit__(exc_type, exc_val, exc_tb)\n",
    "        tmpdir = self.tmpdir.name\n",
    "        # Remove it from sys.path\n",
    "        sys.path.remove(tmpdir)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "- [\"Coin flip\" sample](#coin-flip-sample) (via GitHub)\n",
    "- [\"Coin flip\" sample](#coin.tar.gz) (via Google Cloud Storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Coin flip\" sample\n",
    "- wrap [the \"coin flip\" example from the Kubeflow Pipelines repo](https://github.com/kubeflow/pipelines/blob/0.1.31/samples/core/condition/condition.py) in a `ScheduledWorkflow`\n",
    "- create it, let it run for a few minutes, update (\"patch\") it, and delete it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `import` pipeline definition from latest GitHub release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_flip_sample_url = 'https://raw.githubusercontent.com/kubeflow/pipelines/%s/samples/core/condition/condition.py' % kfp_version\n",
    "with Import(coin_flip_sample_url):\n",
    "    # The condition.py has been downloaded to a temporary directory and placed on sys.path\n",
    "    from condition import flipcoin_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a `ScheduledWorkflow` wrapping the pipeline\n",
    "Build a `ScheduledWorkflow` spec that runs the \"coin flip\" pipeline every minute, on the minute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "swf = make_swf_kfp(flipcoin_pipeline, cron=\"0 * * * * *\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create, Read, Update, and Delete the `ScheduledWorkflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize a k8s client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client, config\n",
    "config.load_kube_config()\n",
    "api = client.CustomObjectsApi()\n",
    "args = dict(\n",
    "    group=\"kubeflow.org\",\n",
    "    version=\"v1beta1\",\n",
    "    namespace=\"default\",\n",
    "    plural=\"scheduledworkflows\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swf-conditional-execution-pipeline'"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = api.create_namespaced_custom_object(body=swf, **args)\n",
    "name = obj['metadata']['name']\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resource = api.get_namespaced_custom_object(name=name, **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['apiVersion', 'kind', 'status', 'spec', 'metadata'])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completed': [{'createdAt': '2019-09-29T21:00:07Z',\n",
       "   'finishedAt': '2019-09-29T21:00:48Z',\n",
       "   'index': 48,\n",
       "   'name': 'swf-conditional-execution-pipeline-48-1519533253',\n",
       "   'namespace': 'default',\n",
       "   'phase': 'Succeeded',\n",
       "   'scheduledAt': '2019-09-29T21:00:00Z',\n",
       "   'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-48-1519533253',\n",
       "   'startedAt': '2019-09-29T21:00:07Z',\n",
       "   'uid': '16baa7fa-b4d6-4a38-8809-aa3c7588d557'},\n",
       "  {'createdAt': '2019-09-29T20:11:37Z',\n",
       "   'finishedAt': '2019-09-29T20:12:11Z',\n",
       "   'index': 47,\n",
       "   'name': 'swf-conditional-execution-pipeline-47-1536310872',\n",
       "   'namespace': 'default',\n",
       "   'phase': 'Succeeded',\n",
       "   'scheduledAt': '2019-09-29T20:00:00Z',\n",
       "   'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-47-1536310872',\n",
       "   'startedAt': '2019-09-29T20:11:37Z',\n",
       "   'uid': '80f3cb60-481d-4716-94ee-735690c055f6'},\n",
       "  {'createdAt': '2019-09-29T20:01:00Z',\n",
       "   'finishedAt': '2019-09-29T20:02:28Z',\n",
       "   'index': 46,\n",
       "   'name': 'swf-conditional-execution-pipeline-46-1553088491',\n",
       "   'namespace': 'default',\n",
       "   'phase': 'Succeeded',\n",
       "   'scheduledAt': '2019-09-29T19:00:00Z',\n",
       "   'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-46-1553088491',\n",
       "   'startedAt': '2019-09-29T20:01:00Z',\n",
       "   'uid': '5ece7a28-1985-485e-a552-8e445dbcb33e'},\n",
       "  {'createdAt': '2019-09-29T20:00:50Z',\n",
       "   'finishedAt': '2019-09-29T20:02:28Z',\n",
       "   'index': 45,\n",
       "   'name': 'swf-conditional-execution-pipeline-45-1569866110',\n",
       "   'namespace': 'default',\n",
       "   'phase': 'Succeeded',\n",
       "   'scheduledAt': '2019-09-29T18:00:00Z',\n",
       "   'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-45-1569866110',\n",
       "   'startedAt': '2019-09-29T20:00:50Z',\n",
       "   'uid': '91c7188f-7a19-4701-b65a-b9a155fd005a'},\n",
       "  {'createdAt': '2019-09-29T20:00:40Z',\n",
       "   'finishedAt': '2019-09-29T20:02:27Z',\n",
       "   'index': 44,\n",
       "   'name': 'swf-conditional-execution-pipeline-44-1586643729',\n",
       "   'namespace': 'default',\n",
       "   'phase': 'Succeeded',\n",
       "   'scheduledAt': '2019-09-29T17:00:00Z',\n",
       "   'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-44-1586643729',\n",
       "   'startedAt': '2019-09-29T20:00:40Z',\n",
       "   'uid': '5fa5f60b-8169-4d8c-b638-2956629bfe73'},\n",
       "  {'createdAt': '2019-09-29T20:00:30Z',\n",
       "   'finishedAt': '2019-09-29T20:02:23Z',\n",
       "   'index': 43,\n",
       "   'name': 'swf-conditional-execution-pipeline-43-1603421348',\n",
       "   'namespace': 'default',\n",
       "   'phase': 'Succeeded',\n",
       "   'scheduledAt': '2019-09-29T16:00:00Z',\n",
       "   'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-43-1603421348',\n",
       "   'startedAt': '2019-09-29T20:00:30Z',\n",
       "   'uid': 'b17e4aa3-c7cc-49f3-97e9-26a1014b5111'},\n",
       "  {'createdAt': '2019-09-29T20:00:19Z',\n",
       "   'finishedAt': '2019-09-29T20:02:13Z',\n",
       "   'index': 42,\n",
       "   'name': 'swf-conditional-execution-pipeline-42-1620198967',\n",
       "   'namespace': 'default',\n",
       "   'phase': 'Succeeded',\n",
       "   'scheduledAt': '2019-09-29T15:00:00Z',\n",
       "   'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-42-1620198967',\n",
       "   'startedAt': '2019-09-29T20:00:19Z',\n",
       "   'uid': 'd34e26c0-e51f-49c1-a6c2-f1c559c5fd56'},\n",
       "  {'createdAt': '2019-09-29T20:00:08Z',\n",
       "   'finishedAt': '2019-09-29T20:01:54Z',\n",
       "   'index': 41,\n",
       "   'name': 'swf-conditional-execution-pipeline-41-1636976586',\n",
       "   'namespace': 'default',\n",
       "   'phase': 'Succeeded',\n",
       "   'scheduledAt': '2019-09-29T14:00:00Z',\n",
       "   'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-41-1636976586',\n",
       "   'startedAt': '2019-09-29T20:00:08Z',\n",
       "   'uid': '8e2d7bf8-2a86-4219-a7bd-b08ee4194e7d'},\n",
       "  {'createdAt': '2019-09-29T19:59:58Z',\n",
       "   'finishedAt': '2019-09-29T20:00:47Z',\n",
       "   'index': 40,\n",
       "   'name': 'swf-conditional-execution-pipeline-40-1653754205',\n",
       "   'namespace': 'default',\n",
       "   'phase': 'Succeeded',\n",
       "   'scheduledAt': '2019-09-29T13:00:00Z',\n",
       "   'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-40-1653754205',\n",
       "   'startedAt': '2019-09-29T19:59:58Z',\n",
       "   'uid': 'c03c8c38-7c33-4980-af87-c7cd303a52dd'},\n",
       "  {'createdAt': '2019-09-29T19:59:48Z',\n",
       "   'finishedAt': '2019-09-29T20:00:27Z',\n",
       "   'index': 39,\n",
       "   'name': 'swf-conditional-execution-pipeline-39-1502314349',\n",
       "   'namespace': 'default',\n",
       "   'phase': 'Succeeded',\n",
       "   'scheduledAt': '2019-09-29T12:00:00Z',\n",
       "   'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-39-1502314349',\n",
       "   'startedAt': '2019-09-29T19:59:48Z',\n",
       "   'uid': '6a44ab74-4662-4b03-85f2-d866370d9b1c'}]}"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = resource['status']['workflowHistory']; history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ryan/.pyenv/versions/3.7.3/bin/pip\r\n",
      "/Users/ryan/.pyenv/versions/3.7.3/bin/pip\r\n",
      "/Users/ryan/.pyenv/shims/pip\r\n",
      "/usr/local/bin/pip\r\n"
     ]
    }
   ],
   "source": [
    "!which -a pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas in /Users/ryan/.pyenv/versions/3.7.3/lib/python3.7/site-packages (0.25.1)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/ryan/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pandas) (1.17.0)\r\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /Users/ryan/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pandas) (2019.1)\r\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /Users/ryan/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pandas) (2.8.0)\r\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /Users/ryan/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/ryan/.pyenv/shims/pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-495-7dd3504c366f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ryan/.pyenv/versions/kf-pipelines-py3/bin/python'"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = history['completed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['completed'])"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(completed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update (\"patch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make any desired change to the ScheduledWorkflow body here:\n",
    "swf = make_swf_kfp(flipcoin_pipeline, cron=\"0 0 * * * *\", start='2019-09-28', end='2019-09-30 19:45')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apiVersion': 'kubeflow.org/v1beta1',\n",
       " 'kind': 'ScheduledWorkflow',\n",
       " 'metadata': {'creationTimestamp': '2019-09-29T19:40:27Z',\n",
       "  'generation': 141,\n",
       "  'labels': {'scheduledworkflows.kubeflow.org/enabled': 'true',\n",
       "   'scheduledworkflows.kubeflow.org/status': 'Enabled'},\n",
       "  'name': 'swf-conditional-execution-pipeline',\n",
       "  'namespace': 'default',\n",
       "  'resourceVersion': '231605',\n",
       "  'selfLink': '/apis/kubeflow.org/v1beta1/namespaces/default/scheduledworkflows/swf-conditional-execution-pipeline',\n",
       "  'uid': '384c2441-56ef-4b9f-883a-6c3b485dd8b4'},\n",
       " 'spec': {'description': 'ScheduledWorkflow (cron: 0 0 * * * *, start: 2019-09-28, end: 2019-09-30 19:45): Shows how to use dsl.Condition().',\n",
       "  'enabled': True,\n",
       "  'maxConcurrency': 10,\n",
       "  'maxHistory': 10,\n",
       "  'name': 'swf-conditional-execution-pipeline',\n",
       "  'trigger': {'cronSchedule': {'cron': '0 0 * * * *',\n",
       "    'endTime': '2019-09-30T19:45:00+00:00',\n",
       "    'startTime': '2019-09-28T00:00:00+00:00'}},\n",
       "  'workflow': {'parameters': [{'name': 'datetime',\n",
       "     'value': '[[ScheduledTime.20060102-15:04:05]]'},\n",
       "    {'name': 'pipeline_yaml',\n",
       "     'value': 'apiVersion: argoproj.io/v1alpha1\\nkind: Workflow\\nmetadata:\\n  annotations: {pipelines.kubeflow.org/pipeline_spec: \\'{\"description\": \"Shows how\\n      to use dsl.Condition().\", \"name\": \"Conditional execution pipeline\"}\\'}\\n  generateName: conditional-execution-pipeline-\\nspec:\\n  arguments:\\n    parameters: []\\n  entrypoint: conditional-execution-pipeline\\n  serviceAccountName: pipeline-runner\\n  templates:\\n  - dag:\\n      tasks:\\n      - arguments:\\n          parameters:\\n          - {name: generate-random-number-output, value: \\'{{tasks.generate-random-number.outputs.parameters.generate-random-number-output}}\\'}\\n        dependencies: [generate-random-number]\\n        name: condition-2\\n        template: condition-2\\n        when: \\'{{tasks.generate-random-number.outputs.parameters.generate-random-number-output}}\\n          > 5\\'\\n      - arguments:\\n          parameters:\\n          - {name: generate-random-number-output, value: \\'{{tasks.generate-random-number.outputs.parameters.generate-random-number-output}}\\'}\\n        dependencies: [generate-random-number]\\n        name: condition-3\\n        template: condition-3\\n        when: \\'{{tasks.generate-random-number.outputs.parameters.generate-random-number-output}}\\n          <= 5\\'\\n      - {name: generate-random-number, template: generate-random-number}\\n    name: condition-1\\n  - dag:\\n      tasks:\\n      - arguments:\\n          parameters:\\n          - {name: generate-random-number-output, value: \\'{{inputs.parameters.generate-random-number-output}}\\'}\\n        name: print\\n        template: print\\n    inputs:\\n      parameters:\\n      - {name: generate-random-number-output}\\n    name: condition-2\\n  - dag:\\n      tasks:\\n      - arguments:\\n          parameters:\\n          - {name: generate-random-number-output, value: \\'{{inputs.parameters.generate-random-number-output}}\\'}\\n        name: print-2\\n        template: print-2\\n    inputs:\\n      parameters:\\n      - {name: generate-random-number-output}\\n    name: condition-3\\n  - dag:\\n      tasks:\\n      - arguments:\\n          parameters:\\n          - {name: generate-random-number-2-output, value: \\'{{tasks.generate-random-number-2.outputs.parameters.generate-random-number-2-output}}\\'}\\n        dependencies: [generate-random-number-2]\\n        name: condition-5\\n        template: condition-5\\n        when: \\'{{tasks.generate-random-number-2.outputs.parameters.generate-random-number-2-output}}\\n          > 15\\'\\n      - arguments:\\n          parameters:\\n          - {name: generate-random-number-2-output, value: \\'{{tasks.generate-random-number-2.outputs.parameters.generate-random-number-2-output}}\\'}\\n        dependencies: [generate-random-number-2]\\n        name: condition-6\\n        template: condition-6\\n        when: \\'{{tasks.generate-random-number-2.outputs.parameters.generate-random-number-2-output}}\\n          <= 15\\'\\n      - {name: generate-random-number-2, template: generate-random-number-2}\\n    name: condition-4\\n  - dag:\\n      tasks:\\n      - arguments:\\n          parameters:\\n          - {name: generate-random-number-2-output, value: \\'{{inputs.parameters.generate-random-number-2-output}}\\'}\\n        name: print-3\\n        template: print-3\\n    inputs:\\n      parameters:\\n      - {name: generate-random-number-2-output}\\n    name: condition-5\\n  - dag:\\n      tasks:\\n      - arguments:\\n          parameters:\\n          - {name: generate-random-number-2-output, value: \\'{{inputs.parameters.generate-random-number-2-output}}\\'}\\n        name: print-4\\n        template: print-4\\n    inputs:\\n      parameters:\\n      - {name: generate-random-number-2-output}\\n    name: condition-6\\n  - dag:\\n      tasks:\\n      - dependencies: [flip-coin]\\n        name: condition-1\\n        template: condition-1\\n        when: \\'\"{{tasks.flip-coin.outputs.parameters.flip-coin-output}}\" == \"heads\"\\'\\n      - dependencies: [flip-coin]\\n        name: condition-4\\n        template: condition-4\\n        when: \\'\"{{tasks.flip-coin.outputs.parameters.flip-coin-output}}\" == \"tails\"\\'\\n      - {name: flip-coin, template: flip-coin}\\n    name: conditional-execution-pipeline\\n  - container:\\n      args: [\\'python -c \"import random; result = \\'\\'heads\\'\\' if random.randint(0,1)\\n          == 0 else \\'\\'tails\\'\\'; print(result)\" | tee /tmp/output\\']\\n      command: [sh, -c]\\n      image: python:alpine3.6\\n    name: flip-coin\\n    outputs:\\n      artifacts:\\n      - {name: flip-coin-output, path: /tmp/output}\\n      parameters:\\n      - name: flip-coin-output\\n        valueFrom: {path: /tmp/output}\\n  - container:\\n      args: [\\'python -c \"import random; print(random.randint($0, $1))\" | tee $2\\',\\n        \\'0\\', \\'9\\', /tmp/output]\\n      command: [sh, -c]\\n      image: python:alpine3.6\\n    name: generate-random-number\\n    outputs:\\n      artifacts:\\n      - {name: generate-random-number-output, path: /tmp/output}\\n      parameters:\\n      - name: generate-random-number-output\\n        valueFrom: {path: /tmp/output}\\n  - container:\\n      args: [\\'python -c \"import random; print(random.randint($0, $1))\" | tee $2\\',\\n        \\'10\\', \\'19\\', /tmp/output]\\n      command: [sh, -c]\\n      image: python:alpine3.6\\n    name: generate-random-number-2\\n    outputs:\\n      artifacts:\\n      - {name: generate-random-number-2-output, path: /tmp/output}\\n      parameters:\\n      - name: generate-random-number-2-output\\n        valueFrom: {path: /tmp/output}\\n  - container:\\n      command: [echo, \\'heads and {{inputs.parameters.generate-random-number-output}}\\n          > 5!\\']\\n      image: alpine:3.6\\n    inputs:\\n      parameters:\\n      - {name: generate-random-number-output}\\n    name: print\\n  - container:\\n      command: [echo, \\'heads and {{inputs.parameters.generate-random-number-output}}\\n          <= 5!\\']\\n      image: alpine:3.6\\n    inputs:\\n      parameters:\\n      - {name: generate-random-number-output}\\n    name: print-2\\n  - container:\\n      command: [echo, \\'tails and {{inputs.parameters.generate-random-number-2-output}}\\n          > 15!\\']\\n      image: alpine:3.6\\n    inputs:\\n      parameters:\\n      - {name: generate-random-number-2-output}\\n    name: print-3\\n  - container:\\n      command: [echo, \\'tails and {{inputs.parameters.generate-random-number-2-output}}\\n          <= 15!\\']\\n      image: alpine:3.6\\n    inputs:\\n      parameters:\\n      - {name: generate-random-number-2-output}\\n    name: print-4\\n'}],\n",
       "   'spec': {'arguments': {'parameters': [{'name': 'datetime'},\n",
       "      {'name': 'pipeline_yaml'}]},\n",
       "    'entrypoint': 'kfp',\n",
       "    'templates': [{'container': {'args': ['--datetime',\n",
       "        '{{inputs.parameters.datetime}}',\n",
       "        '--pipeline_yaml',\n",
       "        '{{inputs.parameters.pipeline_yaml}}',\n",
       "        '--name',\n",
       "        'conditional-execution-pipeline'],\n",
       "       'command': ['python', 'swf_kfp_runner.py'],\n",
       "       'image': 'swf-kfp:3'},\n",
       "      'inputs': {'parameters': [{'name': 'datetime'},\n",
       "        {'name': 'pipeline_yaml'}]},\n",
       "      'name': 'kfp'}]}}},\n",
       " 'status': {'conditions': [{'lastHeartbeatTime': '2019-09-29T20:11:27Z',\n",
       "    'lastTransitionTime': '2019-09-29T20:11:27Z',\n",
       "    'message': 'The schedule is enabled.',\n",
       "    'reason': 'Enabled',\n",
       "    'status': 'True',\n",
       "    'type': 'Enabled'}],\n",
       "  'trigger': {'lastTriggeredTime': '2019-09-29T19:00:00Z',\n",
       "   'lastWorkflowIndex': 46},\n",
       "  'workflowHistory': {'completed': [{'createdAt': '2019-09-29T20:01:00Z',\n",
       "     'finishedAt': '2019-09-29T20:02:28Z',\n",
       "     'index': 46,\n",
       "     'name': 'swf-conditional-execution-pipeline-46-1553088491',\n",
       "     'namespace': 'default',\n",
       "     'phase': 'Succeeded',\n",
       "     'scheduledAt': '2019-09-29T19:00:00Z',\n",
       "     'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-46-1553088491',\n",
       "     'startedAt': '2019-09-29T20:01:00Z',\n",
       "     'uid': '5ece7a28-1985-485e-a552-8e445dbcb33e'},\n",
       "    {'createdAt': '2019-09-29T20:00:50Z',\n",
       "     'finishedAt': '2019-09-29T20:02:28Z',\n",
       "     'index': 45,\n",
       "     'name': 'swf-conditional-execution-pipeline-45-1569866110',\n",
       "     'namespace': 'default',\n",
       "     'phase': 'Succeeded',\n",
       "     'scheduledAt': '2019-09-29T18:00:00Z',\n",
       "     'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-45-1569866110',\n",
       "     'startedAt': '2019-09-29T20:00:50Z',\n",
       "     'uid': '91c7188f-7a19-4701-b65a-b9a155fd005a'},\n",
       "    {'createdAt': '2019-09-29T20:00:40Z',\n",
       "     'finishedAt': '2019-09-29T20:02:27Z',\n",
       "     'index': 44,\n",
       "     'name': 'swf-conditional-execution-pipeline-44-1586643729',\n",
       "     'namespace': 'default',\n",
       "     'phase': 'Succeeded',\n",
       "     'scheduledAt': '2019-09-29T17:00:00Z',\n",
       "     'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-44-1586643729',\n",
       "     'startedAt': '2019-09-29T20:00:40Z',\n",
       "     'uid': '5fa5f60b-8169-4d8c-b638-2956629bfe73'},\n",
       "    {'createdAt': '2019-09-29T20:00:30Z',\n",
       "     'finishedAt': '2019-09-29T20:02:23Z',\n",
       "     'index': 43,\n",
       "     'name': 'swf-conditional-execution-pipeline-43-1603421348',\n",
       "     'namespace': 'default',\n",
       "     'phase': 'Succeeded',\n",
       "     'scheduledAt': '2019-09-29T16:00:00Z',\n",
       "     'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-43-1603421348',\n",
       "     'startedAt': '2019-09-29T20:00:30Z',\n",
       "     'uid': 'b17e4aa3-c7cc-49f3-97e9-26a1014b5111'},\n",
       "    {'createdAt': '2019-09-29T20:00:19Z',\n",
       "     'finishedAt': '2019-09-29T20:02:13Z',\n",
       "     'index': 42,\n",
       "     'name': 'swf-conditional-execution-pipeline-42-1620198967',\n",
       "     'namespace': 'default',\n",
       "     'phase': 'Succeeded',\n",
       "     'scheduledAt': '2019-09-29T15:00:00Z',\n",
       "     'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-42-1620198967',\n",
       "     'startedAt': '2019-09-29T20:00:19Z',\n",
       "     'uid': 'd34e26c0-e51f-49c1-a6c2-f1c559c5fd56'},\n",
       "    {'createdAt': '2019-09-29T20:00:08Z',\n",
       "     'finishedAt': '2019-09-29T20:01:54Z',\n",
       "     'index': 41,\n",
       "     'name': 'swf-conditional-execution-pipeline-41-1636976586',\n",
       "     'namespace': 'default',\n",
       "     'phase': 'Succeeded',\n",
       "     'scheduledAt': '2019-09-29T14:00:00Z',\n",
       "     'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-41-1636976586',\n",
       "     'startedAt': '2019-09-29T20:00:08Z',\n",
       "     'uid': '8e2d7bf8-2a86-4219-a7bd-b08ee4194e7d'},\n",
       "    {'createdAt': '2019-09-29T19:59:58Z',\n",
       "     'finishedAt': '2019-09-29T20:00:47Z',\n",
       "     'index': 40,\n",
       "     'name': 'swf-conditional-execution-pipeline-40-1653754205',\n",
       "     'namespace': 'default',\n",
       "     'phase': 'Succeeded',\n",
       "     'scheduledAt': '2019-09-29T13:00:00Z',\n",
       "     'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-40-1653754205',\n",
       "     'startedAt': '2019-09-29T19:59:58Z',\n",
       "     'uid': 'c03c8c38-7c33-4980-af87-c7cd303a52dd'},\n",
       "    {'createdAt': '2019-09-29T19:59:48Z',\n",
       "     'finishedAt': '2019-09-29T20:00:27Z',\n",
       "     'index': 39,\n",
       "     'name': 'swf-conditional-execution-pipeline-39-1502314349',\n",
       "     'namespace': 'default',\n",
       "     'phase': 'Succeeded',\n",
       "     'scheduledAt': '2019-09-29T12:00:00Z',\n",
       "     'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-39-1502314349',\n",
       "     'startedAt': '2019-09-29T19:59:48Z',\n",
       "     'uid': '6a44ab74-4662-4b03-85f2-d866370d9b1c'},\n",
       "    {'createdAt': '2019-09-29T19:59:38Z',\n",
       "     'finishedAt': '2019-09-29T20:00:13Z',\n",
       "     'index': 38,\n",
       "     'name': 'swf-conditional-execution-pipeline-38-1485536730',\n",
       "     'namespace': 'default',\n",
       "     'phase': 'Succeeded',\n",
       "     'scheduledAt': '2019-09-29T11:00:00Z',\n",
       "     'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-38-1485536730',\n",
       "     'startedAt': '2019-09-29T19:59:38Z',\n",
       "     'uid': '37f2c0dd-f250-497f-a748-3c70cf100330'},\n",
       "    {'createdAt': '2019-09-29T19:59:28Z',\n",
       "     'finishedAt': '2019-09-29T20:00:05Z',\n",
       "     'index': 37,\n",
       "     'name': 'swf-conditional-execution-pipeline-37-1267427683',\n",
       "     'namespace': 'default',\n",
       "     'phase': 'Succeeded',\n",
       "     'scheduledAt': '2019-09-29T10:00:00Z',\n",
       "     'selfLink': '/apis/argoproj.io/v1alpha1/namespaces/default/workflows/swf-conditional-execution-pipeline-37-1267427683',\n",
       "     'startedAt': '2019-09-29T19:59:28Z',\n",
       "     'uid': '01fcc0a8-6b78-40f6-aced-272d4ad558e8'}]}}}"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit new ScheduledWorkflow\n",
    "api.patch_namespaced_custom_object(name=name, body=swf, **args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apiVersion': 'v1',\n",
       " 'details': {'group': 'kubeflow.org',\n",
       "  'kind': 'scheduledworkflows',\n",
       "  'name': 'swf-conditional-execution-pipeline',\n",
       "  'uid': '1d7b3943-795f-4b65-b67d-865c8ac10e3f'},\n",
       " 'kind': 'Status',\n",
       " 'metadata': {},\n",
       " 'status': 'Success'}"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.delete_namespaced_custom_object(name=name, body=client.V1DeleteOptions(), **args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `coin.tar.gz`\n",
    "\n",
    "### via `gs`-scheme URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'gs://ml-pipeline-playground/coin.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### via `storage.googleapis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = 'https://storage.googleapis.com/ml-pipeline-playground/coin.tar.gz'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kf-pipelines-py3",
   "language": "python",
   "name": "kf-pipelines-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
